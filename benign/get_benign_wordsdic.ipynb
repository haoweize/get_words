{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ab1ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import zhconv\n",
    "\n",
    "# 准备文本数据总数组，里面每个数组都是一个样例中的文本内容\n",
    "data_list = []\n",
    "# 读取文件\n",
    "# file_data来源于2017中国网络安全技术对抗赛《恶意网页分析》赛题的样本数据集\n",
    "filedir = r'file_data/恶意网页分析/file1'\n",
    "filename = os.listdir(filedir)\n",
    "\n",
    "# 导入file_Dataframe\n",
    "file_df = pd.read_csv(r\"file_url.csv\")\n",
    "file_df['text'] = ''\n",
    "err = 0\n",
    "# 扫描文件列表，处理列表中的文件，提取文件中的html中的纯文本（只保留汉字）， 将文本拼接到file_url.csv文件中形成分词前的数据\n",
    "# 这里的file_url.csv是从file文件中提取出来的url及其标签等信息。\n",
    "for i in range(len(filename)):\n",
    "    sub_filename = filedir+'/'+filename[i]\n",
    "    # 找到filename所在的行\n",
    "    if file_df.index[file_df['id'] == filename[i]].size > 0:\n",
    "        row_index = file_df.index[(file_df['id'] == filename[i])][0]\n",
    "\n",
    "        try:\n",
    "            with open(sub_filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                obj = f.read()\n",
    "                text = obj\n",
    "                # 从html中提取文本信息\n",
    "                soup = BeautifulSoup(text, 'lxml')\n",
    "                file_list = soup.text.split(\" \")\n",
    "                txt = \"\".join(file_list)\n",
    "                # 只保留文本信息中的汉字和字母\n",
    "                txt = re.sub(r'[^\\u4e00-\\u9fa5]', '', txt)\n",
    "                # 繁体字转化成简体字\n",
    "                txt = zhconv.convert(str(txt), 'zh-hans')\n",
    "                file_df.loc[row_index, 'pagetxt'] = txt\n",
    "        except:\n",
    "            err = err+1\n",
    "            pass\n",
    "    # data_list.append(file_list)\n",
    "print(err)\n",
    "file_df.to_csv(\"benign.csv\", encoding=\"utf_8_sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf22d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'benign.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a1cb4",
   "metadata": {},
   "source": [
    "使用哈工大的停用词表以及jieba工具进行词切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "106bac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "def get_word(text, word_dic, stop_words):\n",
    "  # 将文本切分为单个词\n",
    "  words = jieba.cut(text)\n",
    "  for w in words:\n",
    "    # 构建词表\n",
    "    if w in stop_words:\n",
    "      continue\n",
    "    if w in word_dic:\n",
    "      word_dic[w] += 1\n",
    "    else:\n",
    "      word_dic[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fcd781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stopwords():\n",
    "  stop_words = []\n",
    "#这里自己更改文件所在名\n",
    "  with open(r\"hit_stopwords.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    for w in f.read().splitlines():\n",
    "      stop_words.append(w)\n",
    "  return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f5c8e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Hs\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.388 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 生成词表字典\n",
    "corpus = []\n",
    "stop_words = read_stopwords()\n",
    "word_dic = {}\n",
    "df['text'] = df['pagetxt'].apply(lambda i : get_word(str(i), word_dic, stop_words))\n",
    "word_list = sorted(word_dic.items(), key=lambda item: item[1], reverse=True)\n",
    "print(type(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833b6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('benign_word.txt', 'w', encoding='utf-8') as f:\n",
    "    for word, freq in word_list:\n",
    "        f.write(word + \" \" + str(freq) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74818d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
